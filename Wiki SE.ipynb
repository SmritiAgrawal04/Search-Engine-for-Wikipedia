{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.sax\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles= {}\n",
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\", \"align\", \"realign\", \"valign\", \"nonalign\", \"malign\", \"unalign\", \"salign\", \"qalign\", \"halign\", \"font\", \"fontsiz\", \"fontcolor\", \"backgroundcolor\", \"background\", \"style\", \"center\", \"text\"])\n",
    "EXTENDED_PUNCTUATIONS = set(list(string.punctuation) + ['\\n', '\\t', ' '])\n",
    "# INT_DIGITS = set([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "\n",
    "docId=1\n",
    "Wiki_Dict= {}\n",
    "Doc_Dict= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invertedIndex():\n",
    "    global current_invertedFileIndex\n",
    "    sorted(Wiki_Dict.items(), key=lambda t: t[0])\n",
    "    f = open(\"InvertedIndex.txt\", \"w\")\n",
    "    for key, value in Wiki_Dict.items():\n",
    "        line= key +':'+ value +\"\\n\"\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "    Wiki_Dict.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Page:\n",
    "def ProcessPage(pageID, content, index):\n",
    "    for word in content:\n",
    "        if word not in Doc_Dict:\n",
    "            value= np.zeros(6, dtype= int)\n",
    "            Doc_Dict[word]= value\n",
    "            Doc_Dict[word][index] =1\n",
    "        else:\n",
    "            Doc_Dict[word][index] +=1\n",
    "    \n",
    "    if(index ==5):\n",
    "        for key, value in Doc_Dict.items():\n",
    "            if key not in Wiki_Dict:                # pageID_totalcount:t1i2c3b4r5l6|pageID_totalcount:t1i2c3b4r5l6\n",
    "                Wiki_Dict[key]= str(pageID)+ '-'\n",
    "                if value[0] >0:\n",
    "                    Wiki_Dict[key] += 't'+ str(value[0])\n",
    "                if value[1] >0:\n",
    "                    Wiki_Dict[key] += 'i'+ str(value[1])\n",
    "                if value[2] >0:\n",
    "                    Wiki_Dict[key] += 'c'+ str(value[2])\n",
    "                if value[3] >0:\n",
    "                    Wiki_Dict[key] += 'b'+ str(value[3])\n",
    "                if value[4] >0:\n",
    "                    Wiki_Dict[key] += 'r'+ str(value[4])\n",
    "                if value[5] >0:\n",
    "                    Wiki_Dict[key] += 'l'+ str(value[5])\n",
    "            else:\n",
    "                Wiki_Dict[key] += '|'+ str(pageID)+ '-'\n",
    "                if value[0] >0:\n",
    "                    Wiki_Dict[key] += 't'+ str(value[0])\n",
    "                if value[1] >0:\n",
    "                    Wiki_Dict[key] += 'i'+ str(value[1])\n",
    "                if value[2] >0:\n",
    "                    Wiki_Dict[key] += 'c'+ str(value[2])\n",
    "                if value[3] >0:\n",
    "                    Wiki_Dict[key] += 'b'+ str(value[3])\n",
    "                if value[4] >0:\n",
    "                    Wiki_Dict[key] += 'r'+ str(value[4])\n",
    "                if value[5] >0:\n",
    "                    Wiki_Dict[key] += 'l'+ str(value[5])\n",
    "        Doc_Dict.clear()   \n",
    "    \n",
    "def extract_pageDetails(text):\n",
    "    lines= text.split('\\n')\n",
    "    line =0\n",
    "    body_flag= False\n",
    "    infoBox, category, body, references, links= [], [], [], [], []\n",
    "\n",
    "    while line <len(lines):\n",
    "        if \"{{infobox\" in lines[line]:\n",
    "            openBrackets, closedBrackets= 0,0\n",
    "            while line< len(lines):\n",
    "                if \"{{\" in lines[line]:\n",
    "                    openBrackets +=1\n",
    "                if \"}}\" in lines[line]:\n",
    "                    closedBrackets +=1\n",
    "\n",
    "                infoBox.extend(lines[line].split())\n",
    "                if(openBrackets == closedBrackets):\n",
    "                    break\n",
    "                line +=1\n",
    "        elif \"[[category:\" in lines[line]:\n",
    "            body_flag= True\n",
    "            try:\n",
    "                category.extend((lines[line].split(':')[1].split(']]')[0]).split())\n",
    "            except:\n",
    "                category.extend((lines[line].split(':')[1]).split())\n",
    "\n",
    "        elif \"==external links==\" in lines[line] or \"== external links ==\" in lines[line]:\n",
    "            body_flag= True\n",
    "            line +=1\n",
    "            while line< len(lines):\n",
    "                if \"*[\" in lines[line] or \"* [\" in lines[line]:\n",
    "                    link= \"\"\n",
    "                    while line< len(lines) and \"]\" not in lines[line]:\n",
    "                        link += lines[line]\n",
    "                        line+=1\n",
    "                    link += lines[line]\n",
    "                    link = link.split('[')\n",
    "                    if(len(link)> 1):\n",
    "                        link= link[1].split(']')[0]\n",
    "                        links.extend(link.split())\n",
    "                elif \"[[category:\" in lines[line]:\n",
    "                    try:\n",
    "                        category.extend((lines[line].split(':')[1].split(']]')[0]).split())\n",
    "                    except:\n",
    "                        category.extend((lines[line].split(':')[1]).split())\n",
    "                line+=1\n",
    "                \n",
    "\n",
    "        elif \"==references==\" in lines[line] or \"== references ==\" in lines[line]:\n",
    "            body_flag= True\n",
    "            line +=1\n",
    "            while line< len(lines):\n",
    "                if \"==\" in lines[line] or \"[[category:\" in lines[line]:\n",
    "                    line -=1\n",
    "                    break\n",
    "                elif \"{{cite\" in lines[line] or \"{{vcite\" in lines[line]:\n",
    "                    cite_title= lines[line].split(\"title=\")\n",
    "                    if(len(cite_title) >1):\n",
    "                        cite_title= cite_title[1].split(\"|\")[0]\n",
    "                        references.extend(cite_title.split())\n",
    "                elif \"{{\" in lines[line] and \"ref\" not in lines[line]:\n",
    "                    references.extend((lines[line].split(\"{{\")[1].split(\"}}\")[0]).split())\n",
    "                line+=1\n",
    "                \n",
    "        elif body_flag== False:\n",
    "            body.extend(lines[line].split())\n",
    "        line +=1  \n",
    "    \n",
    "#     print (infoBox, category, body, references, links)\n",
    "    return ' '.join(infoBox), ' '.join(category), ' '.join(body), ' '.join(references), ' '.join(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.docId = 0\n",
    "        self.title=\"\"\n",
    "        self.text = \"\"\n",
    "        self.infoBox= \"\"\n",
    "        self.category= \"\"\n",
    "        self.body= \"\"\n",
    "        self.references= \"\"\n",
    "        self.links= \"\"\n",
    "        self.stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "    def set_data(self, docId, title, text):\n",
    "        self.docId = docId\n",
    "        self.title= title\n",
    "        self.text = text\n",
    "\n",
    "        self.infoBox, self.category, self.body, self.references, self.links = extract_pageDetails(self.text)\n",
    "        \n",
    "        self.title= self.cleanText(self.title)\n",
    "        self.infoBox= self.cleanText(self.infoBox)\n",
    "        self.category= self.cleanText(self.category)\n",
    "        self.body= self.cleanText(self.body)\n",
    "        self.references= self.cleanText(self.references)\n",
    "        self.links= self.cleanText(self.links)\n",
    "        \n",
    "        \n",
    "        ProcessPage(self.docId, self.title, 0)\n",
    "        ProcessPage(self.docId, self.infoBox, 1)\n",
    "        ProcessPage(self.docId, self.category, 2)\n",
    "        ProcessPage(self.docId, self.body, 3)\n",
    "        ProcessPage(self.docId, self.references, 4)\n",
    "        ProcessPage(self.docId, self.links, 5)\n",
    "    \n",
    "    def cleanText(self, text):\n",
    "        text = re.sub(r'\\\\d+', '', text) #Remove numbers\n",
    "        text = re.sub(r'<(.*?)>','',text) #Remove tags if any\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE) #Remove Url\n",
    "        text = re.sub(r'{\\|(.*?)\\|}', '', text, flags=re.MULTILINE) #Remove CSS\n",
    "        text = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', text, flags=re.MULTILINE) #Remove File\n",
    "        text = re.sub(r'[.,;_()\"/\\'=]', ' ', text, flags=re.MULTILINE) #Remove Punctuaion\n",
    "        text = re.sub(r'[~`!@#$%&-^*+{\\[}\\]()\":\\|\\\\<>/?]', ' ', text, flags=re.MULTILINE)\n",
    "        text = text.split()\n",
    "        text = [x for x in text if x not in STOPWORDS and x not in URL_STOP_WORDS]\n",
    "        text = [self.stemmer.stem(word) for word in text]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiHandler( xml.sax.ContentHandler ):\n",
    "    def __init__(self):\n",
    "        self.CurrentData = \"\"\n",
    "        self.title = \"\"\n",
    "        self.text = \"\"\n",
    "\n",
    "    # Call when an element starts\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.CurrentData = tag\n",
    "#         global docId\n",
    "#         if tag == \"page\":\n",
    "#             print (docId, end= \" \")\n",
    "\n",
    "    # Call when an elements ends\n",
    "    def endElement(self, tag):\n",
    "        global docId\n",
    "        if self.CurrentData == \"title\":\n",
    "            Titles[docId]= self.title\n",
    "#             print (\"title: \", self.title)\n",
    "        elif self.CurrentData == \"text\":\n",
    "            page= Data()\n",
    "            page.set_data(docId, self.title, self.text)\n",
    "            docId +=1\n",
    "        self.text= \"\"\n",
    "        self.CurrentData = \"\"\n",
    "\n",
    "    # Call when a character is read\n",
    "    def characters(self, content):\n",
    "        if self.CurrentData == \"title\":\n",
    "            self.title = content.lower()\n",
    "        elif self.CurrentData == \"text\":\n",
    "            self.text += content.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing Time :  0  hrs  17  mns 42  secs\n"
     ]
    }
   ],
   "source": [
    "if ( __name__ == \"__main__\"):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # create an XMLReader\n",
    "    parser = xml.sax.make_parser()\n",
    "    # turn off namepsaces\n",
    "    parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "\n",
    "    # override the default ContextHandler\n",
    "    Handler = WikiHandler()\n",
    "    parser.setContentHandler(Handler)\n",
    "\n",
    "    dump_data= \"../Phase1/wikipedia.xml\"\n",
    "    parser.parse(dump_data)\n",
    "    \n",
    "    create_invertedIndex()\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    secs  = (end-start).seconds\n",
    "    hr = int(secs/(60*60))\n",
    "    rm = int(secs%(60*60))\n",
    "    mn = int(rm/60)\n",
    "    rm=int(rm%60)\n",
    "    secs = int(rm)\n",
    "    print(\"\\nIndexing Time : \",hr,\" hrs \",mn,\" mns\",secs,\" secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
