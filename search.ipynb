{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, sys, pickle, datetime, math\n",
    "import nltk\n",
    "import Stemmer\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from bisect import bisect_left\n",
    "from heapq import nlargest \n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\", \"align\", \"realign\", \"valign\", \"nonalign\", \"malign\", \"unalign\", \"salign\", \"qalign\", \"halign\", \"font\", \"fontsiz\", \"fontcolor\", \"backgroundcolor\", \"background\", \"style\", \"center\", \"text\"])\n",
    "fields= ['t:', 'i:', 'c:', 'b:', 'r:', 'l:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTitles():\n",
    "    f = open(\"./temp_index/Titles\",'rb')\n",
    "    db = pickle.load(f)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_secondaryIndex():\n",
    "    f = open(\"./temp_index/SecondaryIndex\",'rb')\n",
    "    db = pickle.load(f)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_FieldFrequency(posting_list, current_field):\n",
    "    Fields= ['t', 'b', 'i', 'c', 'r', 'l']\n",
    "    Fields.remove(current_field)\n",
    "    fieldWeights= [0.20, 0.10, 0.10, 0.07, 0.03]\n",
    "    \n",
    "    numOfOccurences= 0\n",
    "    for f in range (0, len(Fields)):\n",
    "        index= posting_list.find(Fields[f])\n",
    "        if index !=-1:\n",
    "            index +=1\n",
    "            count =''\n",
    "            while index< len(posting_list) and posting_list[index].isdigit():\n",
    "                count += posting_list[index]\n",
    "                index +=1\n",
    "            numOfOccurences += int(count) *  fieldWeights[f]\n",
    "    \n",
    "    index= posting_list.find(current_field)\n",
    "    if index !=-1:\n",
    "        index +=1\n",
    "        count =''\n",
    "        while index< len(posting_list) and posting_list[index].isdigit():\n",
    "            count += posting_list[index]\n",
    "            index +=1\n",
    "        numOfOccurences += int(count)*0.5\n",
    "    return numOfOccurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_PlainFrequency(posting_list):\n",
    "    Fields= ['t', 'b', 'i', 'c', 'r', 'l']\n",
    "    \n",
    "    fieldWeights= [0.35, 0.25, 0.20, 0.10, 0.07, 0.03]\n",
    "    \n",
    "    numOfOccurences= 0\n",
    "    for f in range (0, len(Fields)):\n",
    "        index= posting_list.find(Fields[f])\n",
    "        if index !=-1:\n",
    "            index +=1\n",
    "            count =''\n",
    "            while index< len(posting_list) and posting_list[index].isdigit():\n",
    "                count += posting_list[index]\n",
    "                index +=1\n",
    "            numOfOccurences += int(count) *  fieldWeights[f]\n",
    "        \n",
    "    return numOfOccurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateField_tfidf(wordPostings):\n",
    "    tfidfScores ={}\n",
    "    for word in wordPostings.keys():\n",
    "        current_field= wordPostings[word][1]\n",
    "        posting_lists= wordPostings[word][0].split('|')\n",
    "        totalDocs= len(posting_lists)\n",
    "        numOfDocs= 0\n",
    "        for pl in range(0, totalDocs):\n",
    "            if current_field in posting_lists[pl]:\n",
    "                numOfDocs +=1\n",
    "        \n",
    "        for pl in range(0, totalDocs):\n",
    "            if posting_lists[pl] != '':\n",
    "                pageID= posting_lists[pl].split('-')[0]\n",
    "                numOfOccurences= cal_FieldFrequency(posting_lists[pl].split('-')[1], current_field)\n",
    "                totalWordsOfDoc= Titles[int(pageID)][1]\n",
    "                tf = numOfOccurences/totalWordsOfDoc\n",
    "                idf= totalDocs/(numOfDocs+1)\n",
    "                tfidf= tf * math.log10(idf)\n",
    "\n",
    "                if pageID not in tfidfScores:\n",
    "                    tfidfScores[pageID]= tfidf\n",
    "                else:\n",
    "                    tfidfScores[pageID] +=tfidf\n",
    "        \n",
    "    return tfidfScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getField_postingLists(fileWordMap):\n",
    "    wordPostings= {}\n",
    "    for file_num, entry in fileWordMap.items():\n",
    "        file_ptr= open(\"./temp_index/FinalIndex{}\".format(file_num), \"rb\")\n",
    "        data= pickle.load(file_ptr)\n",
    "        for word, field in entry:\n",
    "            if word in data:\n",
    "                wordPostings[word]= (data[word], field)\n",
    "            else:\n",
    "                wordPostings[word]= ('','')\n",
    "        file_ptr.close()\n",
    "        \n",
    "    return wordPostings       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePlain_tfidf(wordPostings):\n",
    "    tfidfScores ={}\n",
    "    for word in wordPostings.keys():\n",
    "        posting_lists= wordPostings[word].split('|')\n",
    "        numOfDocs= len(posting_lists)\n",
    "        totalDocs= len(Titles)\n",
    "        for pl in range(0, numOfDocs):\n",
    "            if posting_lists[pl] != '':\n",
    "                pageID= posting_lists[pl].split('-')[0]\n",
    "                numOfOccurences= cal_PlainFrequency(posting_lists[pl].split('-')[1])\n",
    "                totalWordsOfDoc= Titles[int(pageID)][1]\n",
    "                tf= numOfOccurences/totalWordsOfDoc\n",
    "                idf= totalDocs/(numOfDocs+1)\n",
    "                tfidf= tf * math.log10(idf)\n",
    "\n",
    "                if pageID not in tfidfScores:\n",
    "                    tfidfScores[pageID]= tfidf\n",
    "                else:\n",
    "                    tfidfScores[pageID] +=tfidf\n",
    "      \n",
    "    return tfidfScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlain_postingLists(fileWordMap):\n",
    "    wordPostings= {}\n",
    "    for file_num, words in fileWordMap.items():\n",
    "        file_ptr= open(\"./temp_index/FinalIndex{}\".format(file_num), \"rb\")\n",
    "        data= pickle.load(file_ptr)\n",
    "        for w in words:\n",
    "            if w in data:\n",
    "                wordPostings[w]= data[w]\n",
    "            else:\n",
    "                wordPostings[w]= ''\n",
    "        file_ptr.close()\n",
    "    return wordPostings       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexFile(secondaryIndex_keys, word):\n",
    "    i = bisect_left(secondaryIndex_keys, word)\n",
    "    if i<len(secondaryIndex_keys):\n",
    "        if (secondaryIndex_keys[i] == word):\n",
    "            return i+1\n",
    "        else:\n",
    "            return i\n",
    "    else:\n",
    "          return i               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQuery(query):\n",
    "    stemmer= Stemmer.Stemmer(\"english\")\n",
    "    query= query.lower()\n",
    "    query = re.sub(r'<(.*?)>','',query) #Remove tags if any\n",
    "    query = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', query, flags=re.MULTILINE) #Remove Url\n",
    "    query = re.sub(r'{\\|(.*?)\\|}', '', query, flags=re.MULTILINE) #Remove CSS\n",
    "    query = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', query, flags=re.MULTILINE) #Remove File\n",
    "    query = re.sub(r'[^\\w\\s]' , '', query) #Remove Punctuations & Special Characters\n",
    "    query = query.split()\n",
    "    query = [x for x in query if x not in STOPWORDS and x not in URL_STOP_WORDS and (x.isdigit() and (len(x)<=2 or len(x)>=5)) ==False and bool(re.match('^(?=.*[a-zA-Z])(?=.*[0-9])', x)) ==False and isEnglish(x)] \n",
    "    query = [stemmer.stemWord(word) for word in query]\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fieldQuery(query):\n",
    "    fieldInfo= {}\n",
    "    fileWordMap= {}\n",
    "    \n",
    "    for f in fields:\n",
    "        field= query.find(f)\n",
    "        if field !=-1:\n",
    "            fieldInfo[field]= f\n",
    "    \n",
    "    fieldInfo= sorted(fieldInfo.items())\n",
    "    fieldInfo.append((12345678901234567890123456789, \"\"))            #fake dummy entry\n",
    "    i=0\n",
    "    while i+1 <len(fieldInfo):\n",
    "        field= fieldInfo[i][1].strip(\":\")\n",
    "        fieldQuery = (query[fieldInfo[i][0]+2 : fieldInfo[i+1][0]]).lower()\n",
    "        fieldQuery = cleanQuery(fieldQuery)\n",
    "\n",
    "        for word in fieldQuery:\n",
    "            file_num= get_indexFile(list(secondaryIndex.keys()), word)\n",
    "            if file_num not in fileWordMap:\n",
    "                fileWordMap[file_num] =[(word, field)]\n",
    "            else:\n",
    "                fileWordMap[file_num].append((word, field))\n",
    "        i +=1\n",
    "    return fileWordMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "\n",
      "Searching Time :  0  secs\n",
      "43\n",
      "\n",
      "Searching Time :  0  secs\n",
      "7\n",
      "\n",
      "Searching Time :  0  secs\n"
     ]
    }
   ],
   "source": [
    "Titles= readTitles()\n",
    "secondaryIndex= read_secondaryIndex()\n",
    "queries= open(\"./queries.txt\").readlines()\n",
    "\n",
    "for query in queries:\n",
    "    start = datetime.datetime.now()\n",
    "    query= query.strip('\\n')\n",
    "    K= int(query.split(',')[0])\n",
    "    k=K\n",
    "    query= query.split(',')[1]\n",
    "    fileWordMap= {}\n",
    "    if any(f in query for f in fields): \n",
    "        fileWordMap= process_fieldQuery(query)\n",
    "        tfidfScores= evaluateField_tfidf(getField_postingLists(fileWordMap))\n",
    "        \n",
    "    else:\n",
    "        query = cleanQuery(query)\n",
    "        for word in query:\n",
    "            file_num= get_indexFile(list(secondaryIndex.keys()), word)\n",
    "            if file_num not in fileWordMap:\n",
    "                fileWordMap[file_num] =[word]\n",
    "            else:\n",
    "                fileWordMap[file_num].append(word)\n",
    "                \n",
    "        tfidfScores= evaluatePlain_tfidf(getPlain_postingLists(fileWordMap))\n",
    "        \n",
    "        \n",
    "    tfidfScores= OrderedDict(sorted(tfidfScores.items(), key=lambda t: t[1], reverse= True))\n",
    "    print (len(tfidfScores))\n",
    "    kRelevant= []\n",
    "    for key, value in tfidfScores.items():\n",
    "        if(K ==0):\n",
    "            break\n",
    "        kRelevant.append((key, Titles[int(key)][0]))\n",
    "        K -=1\n",
    "            \n",
    "    end = datetime.datetime.now()\n",
    "    secs  = (end-start).seconds\n",
    "    print(\"\\nSearching Time : \",secs,\" secs\")\n",
    "    \n",
    "    f= open(\"./queries_op.txt\", \"a\")\n",
    "    for pageID, title in kRelevant:\n",
    "        f.write(str(pageID)+\",\"+title+\"\\n\")\n",
    "    f.write(str(secs)+\"\\n\")\n",
    "    f.write(str(secs/k)+\"\\n\\n\")\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in wordPostings.items():\n",
    "#     print (key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in tfidfScores.items():\n",
    "#     print (key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('53', 'list of bluetooth protocols'),\n",
       " ('60', 'autism'),\n",
       " ('161', 'john appold'),\n",
       " ('58', 'wikipedia:articles for deletion/peter delgrosso'),\n",
       " ('198', 'ampeg svt')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kRelevant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
