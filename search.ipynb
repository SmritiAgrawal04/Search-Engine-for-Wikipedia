{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, sys, pickle, datetime, math\n",
    "import nltk\n",
    "import Stemmer\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from bisect import bisect_left\n",
    "from heapq import nlargest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\", \"align\", \"realign\", \"valign\", \"nonalign\", \"malign\", \"unalign\", \"salign\", \"qalign\", \"halign\", \"font\", \"fontsiz\", \"fontcolor\", \"backgroundcolor\", \"background\", \"style\", \"center\", \"text\"])\n",
    "fields= ['t:', 'i:', 'c:', 'b:', 'r:', 'l:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTitles():\n",
    "    f = open(\"./Titles\",'rb')\n",
    "    db = pickle.load(f)\n",
    "    \n",
    "#     for key, value in db.items():\n",
    "#         print (key, \" : \", value)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_secondaryIndex():\n",
    "    f = open(\"./SecondaryIndex\",'rb')\n",
    "    db = pickle.load(f)\n",
    "    \n",
    "#     for key, value in db.items():\n",
    "#         print (key, \" : \", value)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tfidf(wordPostings):\n",
    "    tfidfScores ={}\n",
    "    for word in wordPostings.keys():\n",
    "        posting_lists= wordPostings[word].split('|')\n",
    "        numOfDocs= len(posting_lists)\n",
    "        totalDocs= len(Titles)\n",
    "        for pl in range(0, numOfDocs):\n",
    "            pageID= posting_lists[pl].split('-')[0]\n",
    "            numOfOccurences= int(posting_lists[pl].split('x')[1])\n",
    "            tfidf= math.log10(1+numOfOccurences) * math.log10(totalDocs/(numOfDocs+1))\n",
    "            posting_lists[pl]= posting_lists[pl].split('x')[0]+ 'x'+ str(tfidf)\n",
    "            \n",
    "            if pageID not in tfidfScores:\n",
    "                tfidfScores[pageID]= tfidf\n",
    "            else:\n",
    "                tfidfScores[pageID] += tfidf\n",
    "\n",
    "        wordPostings[word]= '|'.join(posting_lists)\n",
    "        \n",
    "    return wordPostings, dict(sorted(tfidfScores.items(), key=lambda t: t[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postingLists(fileWordMap):\n",
    "    wordPostings= {}\n",
    "    for file_num, words in fileWordMap.items():\n",
    "        file_ptr= open(\"./inverted_index/FinalIndex{}\".format(file_num), \"rb\")\n",
    "        data= pickle.load(file_ptr)\n",
    "        for w in words:\n",
    "            if w in data:\n",
    "                wordPostings[w]= data[w]\n",
    "            else:\n",
    "                wordPostings[w]= ''\n",
    "        file_ptr.close()\n",
    "    return wordPostings       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexFile(secondaryIndex_keys, word):\n",
    "    i = bisect_left(secondaryIndex_keys, word)\n",
    "    if i:\n",
    "        if (secondaryIndex_keys[i] == word):\n",
    "            return i+1\n",
    "        else:\n",
    "            return i\n",
    "    else:\n",
    "          return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQuery(query):\n",
    "    stemmer= Stemmer.Stemmer(\"english\")\n",
    "    query= query.lower()\n",
    "    query = re.sub(r'<(.*?)>','',query) #Remove tags if any\n",
    "    query = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', query, flags=re.MULTILINE) #Remove Url\n",
    "    query = re.sub(r'{\\|(.*?)\\|}', '', query, flags=re.MULTILINE) #Remove CSS\n",
    "    query = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', query, flags=re.MULTILINE) #Remove File\n",
    "    query = re.sub(r'[^\\w\\s]' , '', query) #Remove Punctuations & Special Characters\n",
    "    query = query.split()\n",
    "    query = [x for x in query if x not in STOPWORDS and x not in URL_STOP_WORDS and (x.isdigit() and (len(x)<=2 or len(x)>=5)) ==False and bool(re.match('^(?=.*[a-zA-Z])(?=.*[0-9])', x)) ==False and isEnglish(x)] \n",
    "    query = [stemmer.stemWord(word) for word in query]\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_fieldQuery(documents, field_type):\n",
    "    fieldDocs= []\n",
    "    documents= documents.split(\"|\")\n",
    "    for doc in documents:\n",
    "        if field_type in doc:\n",
    "            fieldDocs.append(doc)\n",
    "    return fieldDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fieldQuery(query):\n",
    "    fieldInfo= {}\n",
    "    \n",
    "    for f in fields:\n",
    "        field= query.find(f)\n",
    "        if field !=-1:\n",
    "            fieldInfo[field]= f\n",
    "    \n",
    "    fieldInfo= sorted(fieldInfo.items())\n",
    "    fieldInfo.append((1234567890, \"\"))\n",
    "    print(fieldInfo)\n",
    "    i=0\n",
    "    while i+1 <len(fieldInfo):\n",
    "        fieldQuery = (query[fieldInfo[i][0]+2 : fieldInfo[i+1][0]]).lower()\n",
    "        fieldQuery = cleanQuery(fieldQuery)\n",
    "        for word in fieldQuery:\n",
    "            if word not in Index:\n",
    "                print (word, \" : \", [])\n",
    "            else:\n",
    "                value= Index[word]\n",
    "                print(word, \" : \", search_fieldQuery(value, fieldInfo[i][1][:1]))\n",
    "            print()\n",
    "        i +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Searching Time :  0  hrs  0  mns 0  secs\n"
     ]
    }
   ],
   "source": [
    "Titles= readTitles()\n",
    "secondaryIndex= read_secondaryIndex()\n",
    "queries= [\"553 slip 1990 jutzi sent kai lash stategeorgia\"]\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "for query in queries:\n",
    "    fileWordMap= {}\n",
    "    kRelevantTitles= []\n",
    "    if any(f in query for f in fields): \n",
    "        process_fieldQuery(query)\n",
    "    else:\n",
    "        query = cleanQuery(query)\n",
    "        for word in query:\n",
    "            file_num= get_indexFile(list(secondaryIndex.keys()), word)\n",
    "            if file_num not in fileWordMap:\n",
    "                fileWordMap[file_num] =[word]\n",
    "            else:\n",
    "                fileWordMap[file_num].append(word)\n",
    "                \n",
    "        wordPostings, tfidfScores= evaluate_tfidf(get_postingLists(fileWordMap))\n",
    "        kRelevant= nlargest(5, tfidfScores, key = tfidfScores.get)\n",
    "        for pageId in kRelevant:\n",
    "            kRelevantTitles.append(Titles[int(pageId)])\n",
    "            \n",
    "    \n",
    "\n",
    "end = datetime.datetime.now()\n",
    "secs  = (end-start).seconds\n",
    "hr = int(secs/(60*60))\n",
    "rm = int(secs%(60*60))\n",
    "mn = int(rm/60)\n",
    "rm=int(rm%60)\n",
    "secs = int(rm)\n",
    "\n",
    "print(\"\\Searching Time : \",hr,\" hrs \",mn,\" mns\",secs,\" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 : 85-b1x0.5308447212559941\n",
      "1990 : 2-i2b8x0.9898589900243991|7-b1x0.28613341703060285|10-i1x0.28613341703060285|16-b3x0.5722668340612057|18-b1x0.28613341703060285|65-b2x0.4535107361967138|70-b4x0.664381219889479|71-b2x0.4535107361967138|72-b3x0.5722668340612057|73-b1x0.28613341703060285|80-b1x0.28613341703060285|85-b1x0.28613341703060285\n",
      "slip : 2-b1x0.5308447212559941\n",
      "sent : 1-b18x1.1743177349632192|2-b1x0.27644486193486334|7-b1x0.27644486193486334|10-i2x0.43815473968379576|13-b3x0.5528897238697267|16-b1x0.27644486193486334|67-b1x0.27644486193486334|69-b5x0.7145996016186591|70-b1x0.27644486193486334|73-b3x0.5528897238697267|85-b1x0.27644486193486334|86-b3x0.5528897238697267|94-b5x0.7145996016186591\n",
      "stategeorgia : 65-b3x1.0616894425119883\n",
      "jutzi : 18-b1x0.5308447212559941\n",
      "kai : 1-b1x0.5308447212559941\n",
      "lash : 69-b1x0.5308447212559941\n"
     ]
    }
   ],
   "source": [
    "for key, value in wordPostings.items():\n",
    "    print (key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : 1.7971485732152568\n",
      "1 : 1.7051624562192134\n",
      "65 : 1.5152001787087022\n",
      "69 : 1.2454443228746532\n",
      "85 : 1.0934230002214604\n",
      "70 : 0.9408260818243424\n",
      "16 : 0.848711695996069\n",
      "73 : 0.8390231409003295\n",
      "18 : 0.816978138286597\n",
      "10 : 0.7242881567143986\n",
      "94 : 0.7145996016186591\n",
      "72 : 0.5722668340612057\n",
      "7 : 0.5625782789654662\n",
      "13 : 0.5528897238697267\n",
      "86 : 0.5528897238697267\n",
      "71 : 0.4535107361967138\n",
      "80 : 0.28613341703060285\n",
      "67 : 0.27644486193486334\n"
     ]
    }
   ],
   "source": [
    "for key, value in tfidfScores.items():\n",
    "    print (key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '1', '65', '69', '85']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kRelavant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['andre agassi', 'apollo', 'alabama', 'abraham lincoln', 'algeria']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kRelevantTitles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
