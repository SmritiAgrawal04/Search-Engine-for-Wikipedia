{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, sys, pickle\n",
    "import nltk\n",
    "import Stemmer\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\", \"align\", \"realign\", \"valign\", \"nonalign\", \"malign\", \"unalign\", \"salign\", \"qalign\", \"halign\", \"font\", \"fontsiz\", \"fontcolor\", \"backgroundcolor\", \"background\", \"style\", \"center\", \"text\"])\n",
    "fields= ['t:', 'i:', 'c:', 'b:', 'r:', 'l:']\n",
    "\n",
    "matches ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_secondaryIndex():\n",
    "#     with open(\"./SecondaryIndex\",\"rb\") as f:\n",
    "    f = open(\"./SecondaryIndex\",'rb')\n",
    "    db = pickle.load(f)\n",
    "    \n",
    "#     for key, value in db.items():\n",
    "#         print (key, \" : \", value)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexFile(secondaryIndex, word):\n",
    "    for key, val in secondaryIndex.items():\n",
    "        if key > word: \n",
    "            break\n",
    "        elif key == word:\n",
    "            return val\n",
    "        prev= val\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readIndex():\n",
    "    f= open(\"./InvertedIndices/InvertedIndex.txt\", \"r\")\n",
    "    Index= {}\n",
    "\n",
    "    lines= f.readlines()\n",
    "    line =0\n",
    "\n",
    "    while line< len(lines):\n",
    "        text = lines[line].split(\":\")\n",
    "        word= text[0]\n",
    "        frequency= text[1]\n",
    "        line +=1\n",
    "        while line< len(lines) and \":\" not in lines[line]:\n",
    "            frequency += lines[line]\n",
    "            line +=1\n",
    "        Index[word]= frequency\n",
    "\n",
    "    f.close()\n",
    "    return Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQuery(query):\n",
    "    stemmer= Stemmer.Stemmer(\"english\")\n",
    "    query = re.sub(r'<(.*?)>','',query) #Remove tags if any\n",
    "    query = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', query, flags=re.MULTILINE) #Remove Url\n",
    "    query = re.sub(r'{\\|(.*?)\\|}', '', query, flags=re.MULTILINE) #Remove CSS\n",
    "    query = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', query, flags=re.MULTILINE) #Remove File\n",
    "    query = re.sub(r'[^\\w\\s]' , '', query) #Remove Punctuations & Special Characters\n",
    "    query = query.split()\n",
    "    query = [x for x in query if x not in STOPWORDS and x not in URL_STOP_WORDS and (x.isdigit() and (len(x)<=2 or len(x)>=5)) ==False and bool(re.match('^(?=.*[a-zA-Z])(?=.*[0-9])', x)) ==False and isEnglish(x)] \n",
    "    query = [stemmer.stemWord(word) for word in query]\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_fieldQuery(documents, field_type):\n",
    "    fieldDocs= []\n",
    "    documents= documents.split(\"|\")\n",
    "    for doc in documents:\n",
    "        if field_type in doc:\n",
    "            fieldDocs.append(doc)\n",
    "    return fieldDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fieldQuery(query):\n",
    "    fieldInfo= {}\n",
    "    \n",
    "    for f in fields:\n",
    "        field= query.find(f)\n",
    "        if field !=-1:\n",
    "            fieldInfo[field]= f\n",
    "    \n",
    "    fieldInfo= sorted(fieldInfo.items())\n",
    "    fieldInfo.append((1234567890, \"\"))\n",
    "    print(fieldInfo)\n",
    "    i=0\n",
    "    while i+1 <len(fieldInfo):\n",
    "        fieldQuery = (query[fieldInfo[i][0]+2 : fieldInfo[i+1][0]]).lower()\n",
    "#         print (fieldQuery)\n",
    "        fieldQuery = cleanQuery(fieldQuery)\n",
    "#         print (fieldQuery)\n",
    "        for word in fieldQuery:\n",
    "            if word not in Index:\n",
    "                print (word, \" : \", [])\n",
    "            else:\n",
    "                value= Index[word]\n",
    "                print(word, \" : \", search_fieldQuery(value, fieldInfo[i][1][:1]))\n",
    "            print()\n",
    "        i +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeroplan : 1\n",
      "foundationtitlemedian : 2\n",
      "000 : 1\n"
     ]
    }
   ],
   "source": [
    "secondaryIndex= read_secondaryIndex()\n",
    "query= \"Aeroplane foundationtitlemedian 000\"\n",
    "\n",
    "if \"t:\" in query or \"i:\" in query or \"c:\" in query or \"b:\"in query or \"r:\" in query or \"l:\" in query:    \n",
    "    process_fieldQuery(query)\n",
    "else:\n",
    "    query = cleanQuery(query)\n",
    "    for word in query:\n",
    "        indexFile= get_indexFile(secondaryIndex, word.lower())\n",
    "        print (word.lower(), \":\", indexFile)\n",
    "        \n",
    "#         Index= readIndex(indexFile)\n",
    "#         if word.lower() in Index:\n",
    "#             print (word, \" : \", Index[word.lower()])\n",
    "#         else:\n",
    "#             print (word, \" : \", [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
