{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.sax\n",
    "import sys, os\n",
    "import csv\n",
    "import re\n",
    "import glob \n",
    "import pickle\n",
    "import string\n",
    "import datetime\n",
    "import Stemmer\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from heapq import heapify, heappush, heappop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\"])\n",
    "UNWANTED_LINES= set([ \"redirect\", \"align\", \"realign\", \"valign\", \"nonalign\", \"malign\", \"unalign\", \"salign\", \"qalign\", \"halign\", \"font\", \"fontsiz\", \"fontcolor\", \"backgroundcolor\", \"background\", \"style\", \"center\", \"text\"])\n",
    "fields= ['t', 'i', 'c', 'b', 'r', 'l']\n",
    "docId=1\n",
    "current_indexNum= 1\n",
    "Wiki_Dict= {}\n",
    "Doc_Dict= {}\n",
    "\n",
    "inverted_FilePath= \"./inverted_index/index{}\"\n",
    "invertedIndex_File= \"./Index\"\n",
    "\n",
    "total_tokens =0\n",
    "stored_tokens =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kWayMerge():\n",
    "    global current_indexNum\n",
    "    numOfFiles= current_indexNum\n",
    "    input_files= []\n",
    "    \n",
    "    for file_num in range (1,numOfFiles):\n",
    "        input_files.append(open(inverted_FilePath.format(str(file_num)), \"r\"))\n",
    "        \n",
    "    output_file= open(invertedIndex_File, \"w\")\n",
    "    \n",
    "    heap = [] \n",
    "    heapify(heap)  \n",
    "    \n",
    "    for file_num in range (1, current_indexNum):\n",
    "        try:\n",
    "            line= input_files[file_num-1].readline()\n",
    "            word= line.split(\":\")[0]\n",
    "            value= line.split(\":\")[1].split(\"\\n\")[0]\n",
    "            heappush(heap, (word, value, file_num))\n",
    "        except:\n",
    "            print (\"No lines is file {}\".format(file_num))\n",
    "            \n",
    "    while heap:\n",
    "        entry= heappop(heap)\n",
    "        line= entry[0]+ \":\"+ entry[1];\n",
    "        file_num= entry[2]\n",
    "        while heap and heap[0][0] == entry[0]:\n",
    "            line += \"|\"+ heap[0][1]\n",
    "            try:\n",
    "                new_line= input_files[heap[0][2]-1].readline()\n",
    "                new_word= new_line.split(\":\")[0]\n",
    "                new_value= new_line.split(\":\")[1].split(\"\\n\")[0]\n",
    "                heappush(heap, (new_word, new_value, heap[0][2]))\n",
    "            except:\n",
    "                print (\"No lines is file {}\".format(heap[0][2]))\n",
    "#                 os.remove(inverted_FilePath.format(str(heap[0][2])))\n",
    "            heappop(heap)\n",
    "        \n",
    "        line +=\"\\n\"\n",
    "        output_file.write(line)\n",
    "        try:\n",
    "            line= input_files[file_num-1].readline()\n",
    "            word= line.split(\":\")[0]\n",
    "            value= line.split(\":\")[1].split(\"\\n\")[0]\n",
    "            heappush(heap, (word, value, file_num))\n",
    "        except:\n",
    "            print (\"No lines is file {}\".format(file_num))\n",
    "#             os.remove(inverted_FilePath.format(str(file_num)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invertedIndex():\n",
    "    global Wiki_Dict, current_indexNum, stored_tokens\n",
    "    sorted_WikiDict= sorted(Wiki_Dict.items(), key=lambda t: t[0])\n",
    "    f = open(inverted_FilePath.format(str(current_indexNum)), \"w\")\n",
    "#     pickle.dump(sorted_WikiDict, f)    \n",
    "    for key, value in sorted_WikiDict:\n",
    "        line= key +':'+ value +\"\\n\"\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "    current_indexNum +=1\n",
    "    stored_tokens += len(Wiki_Dict)\n",
    "    Wiki_Dict.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page:\n",
    "    def ProcessPage(self, pageID, content, index):\n",
    "        for word in content:\n",
    "            if word not in Doc_Dict:\n",
    "                value= np.zeros(6, dtype= int)\n",
    "                Doc_Dict[word]= value\n",
    "                Doc_Dict[word][index] =1\n",
    "            else:\n",
    "                Doc_Dict[word][index] +=1\n",
    "\n",
    "        if(index ==5):\n",
    "            for key, value in Doc_Dict.items():\n",
    "                if key not in Wiki_Dict:                # pageID_totalcount:t1i2c3b4r5l6|pageID_totalcount:t1i2c3b4r5l6\n",
    "                    Wiki_Dict[key]= str(pageID)+ '-'\n",
    "                    for f in range (0, 6):\n",
    "                        if(value[f] >0):\n",
    "                            Wiki_Dict[key] += fields[f] + str(value[f])\n",
    "                else:\n",
    "                    Wiki_Dict[key] += '|'+ str(pageID)+ '-'\n",
    "                    for f in range (0, 6):\n",
    "                        if(value[f] >0):\n",
    "                            Wiki_Dict[key] += fields[f] + str(value[f])\n",
    "            Doc_Dict.clear()   \n",
    "\n",
    "    def extract_pageDetails(self, text):\n",
    "        global total_tokens\n",
    "        lines= text.split('\\n')\n",
    "        line =0\n",
    "        body_flag= False\n",
    "        infoBox, category, body, references, links= [], [], [], [], []\n",
    "\n",
    "        while line <len(lines):\n",
    "            if any(element in lines[line] for element in UNWANTED_LINES) ==True:\n",
    "                total_tokens += len(lines[line].split())\n",
    "                line +=1\n",
    "                \n",
    "            elif \"{{infobox\" in lines[line]:\n",
    "                openBrackets, closedBrackets= 1,0\n",
    "                line +=1\n",
    "                while line< len(lines):\n",
    "                    if \"{{\" in lines[line]:\n",
    "                        openBrackets +=1\n",
    "                    if \"}}\" in lines[line]:\n",
    "                        closedBrackets +=1\n",
    "\n",
    "                    infoBox.extend(lines[line].split())\n",
    "                    if(openBrackets == closedBrackets):\n",
    "                        break\n",
    "                    line +=1\n",
    "                    \n",
    "            elif \"[[category:\" in lines[line]:\n",
    "                body_flag= True\n",
    "                try:\n",
    "                    category.extend((lines[line].split(':')[1].split(']]')[0]).split())\n",
    "                except:\n",
    "                    category.extend((lines[line].split(':')[1]).split())\n",
    "\n",
    "            elif \"==external links==\" in lines[line] or \"== external links ==\" in lines[line]:\n",
    "                body_flag= True\n",
    "                line +=1\n",
    "                while line< len(lines):\n",
    "                    if \"*[\" in lines[line] or \"* [\" in lines[line]:\n",
    "                        link= \"\"\n",
    "                        while line< len(lines) and \"]\" not in lines[line]:\n",
    "                            link += lines[line]\n",
    "                            line+=1\n",
    "                        link += lines[line]\n",
    "                        link = link.split('[')\n",
    "                        if(len(link)> 1):\n",
    "                            link= link[1].split(']')[0]\n",
    "                            links.extend(link.split())\n",
    "                    elif \"[[category:\" in lines[line]:\n",
    "                        try:\n",
    "                            category.extend((lines[line].split(':')[1].split(']]')[0]).split())\n",
    "                        except:\n",
    "                            category.extend((lines[line].split(':')[1]).split())\n",
    "                    line+=1\n",
    "\n",
    "\n",
    "            elif \"==references==\" in lines[line] or \"== references ==\" in lines[line]:\n",
    "                body_flag= True\n",
    "                line +=1\n",
    "                while line< len(lines):\n",
    "                    if \"==\" in lines[line] or \"[[category:\" in lines[line]:\n",
    "                        line -=1\n",
    "                        break\n",
    "                    elif \"{{cite\" in lines[line] or \"{{vcite\" in lines[line]:\n",
    "                        cite_title= lines[line].split(\"title=\")\n",
    "                        if(len(cite_title) >1):\n",
    "                            cite_title= cite_title[1].split(\"|\")[0]\n",
    "                            references.extend(cite_title.split())\n",
    "                    elif \"{{\" in lines[line] and \"ref\" not in lines[line]:\n",
    "                        references.extend((lines[line].split(\"{{\")[1].split(\"}}\")[0]).split())\n",
    "                    line+=1\n",
    "\n",
    "            elif body_flag== False:\n",
    "                body.extend(lines[line].split())\n",
    "            line +=1  \n",
    "\n",
    "        total_tokens+= len(infoBox)+len(category)+len(body)+len(references)+len(links)\n",
    "\n",
    "    #     print (infoBox, category, body, references, links)\n",
    "        return ' '.join(infoBox), ' '.join(category), ' '.join(body), ' '.join(references), ' '.join(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.docId = 0\n",
    "        self.title=\"\"\n",
    "        self.text = \"\"\n",
    "        self.infoBox= \"\"\n",
    "        self.category= \"\"\n",
    "        self.body= \"\"\n",
    "        self.references= \"\"\n",
    "        self.links= \"\"\n",
    "        self.stemmer= Stemmer.Stemmer(\"english\")\n",
    "\n",
    "    def set_data(self, docId, title, text):\n",
    "        self.docId = docId\n",
    "        self.title= title\n",
    "        self.text = text\n",
    "        \n",
    "        page =Page()\n",
    "        self.infoBox, self.category, self.body, self.references, self.links = page.extract_pageDetails(self.text)\n",
    "        \n",
    "        global total_tokens\n",
    "        total_tokens+= len(self.title.split())\n",
    "        \n",
    "        self.title, self.infoBox, self.category, self.body, self.references, self.links= \\\n",
    "        self.cleanText(self.title), self.cleanText(self.infoBox), self.cleanText(self.category), self.cleanText(self.body), self.cleanText(self.references), self.cleanText(self.links)        \n",
    "                \n",
    "        page.ProcessPage(self.docId, self.title, 0)\n",
    "        page.ProcessPage(self.docId, self.infoBox, 1)\n",
    "        page.ProcessPage(self.docId, self.category, 2)\n",
    "        page.ProcessPage(self.docId, self.body, 3)\n",
    "        page.ProcessPage(self.docId, self.references, 4)\n",
    "        page.ProcessPage(self.docId, self.links, 5)\n",
    "        \n",
    "    def isEnglish(self, s):\n",
    "        try:\n",
    "            s.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def cleanText(self, text):\n",
    "#         text = re.sub(r'\\\\d+', '', text) #Remove numbers\n",
    "        text = re.sub(r'<(.*?)>','',text) #Remove tags if any\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE) #Remove Url\n",
    "        text = re.sub(r'{\\|(.*?)\\|}', '', text, flags=re.MULTILINE) #Remove CSS\n",
    "        text = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', text, flags=re.MULTILINE) #Remove File\n",
    "        text = re.sub(r'[^\\w\\s]' , '', text) #Remove Punctuations & Special Characters\n",
    "        text = text.split()\n",
    "        text = [x for x in text if x not in STOPWORDS and x not in URL_STOP_WORDS and (x.isdigit() and (len(x)<=2 or len(x)>=5)) ==False and bool(re.match('^(?=.*[a-zA-Z])(?=.*[0-9])', x)) ==False and self.isEnglish(x)] \n",
    "        text = [self.stemmer.stemWord(word) for word in text]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiHandler( xml.sax.ContentHandler ):\n",
    "    def __init__(self):\n",
    "        self.CurrentData = \"\"\n",
    "        self.title = \"\"\n",
    "        self.text = \"\"\n",
    "\n",
    "    # Call when an element starts\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.CurrentData = tag\n",
    "#         global docId\n",
    "#         if tag == \"page\":\n",
    "#             print (docId, end= \" \")\n",
    "\n",
    "    # Call when an elements ends\n",
    "    def endElement(self, tag):\n",
    "        global docId\n",
    "        if self.CurrentData == \"text\":\n",
    "            data= Data()\n",
    "            data.set_data(docId, self.title, self.text)\n",
    "            docId +=1\n",
    "        self.text= \"\"\n",
    "        self.CurrentData = \"\"\n",
    "\n",
    "    # Call when a character is read\n",
    "    def characters(self, content):\n",
    "        if self.CurrentData == \"title\":\n",
    "            self.title = content.lower()\n",
    "        elif self.CurrentData == \"text\":\n",
    "            self.text += content.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------done a file-------------------------------\n",
      "224821 documents were processed\n",
      "No lines is file 4\n",
      "No lines is file 3\n",
      "No lines is file 1\n",
      "No lines is file 2\n",
      "\n",
      "Total Tokens=  413828885\n",
      "\n",
      "Stored Tokens=  7522831\n",
      "\n",
      "Indexing Time :  1  hrs  2  mns 34  secs\n"
     ]
    }
   ],
   "source": [
    "if ( __name__ == \"__main__\"):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # create an XMLReader\n",
    "    parser = xml.sax.make_parser()\n",
    "    # turn off namepsaces\n",
    "    parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "\n",
    "    # override the default ContextHandler\n",
    "    Handler = WikiHandler()\n",
    "    parser.setContentHandler(Handler)\n",
    "    \n",
    "    files = glob.glob('../Phase2/*.xml') \n",
    "    for file in files:\n",
    "        parser.parse(file)\n",
    "        create_invertedIndex()\n",
    "        print (\"----------------------------------done a file-------------------------------\")\n",
    "        print (\"{} documents were processed\".format(docId))\n",
    "    \n",
    "    kWayMerge()\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    secs  = (end-start).seconds\n",
    "    hr = int(secs/(60*60))\n",
    "    rm = int(secs%(60*60))\n",
    "    mn = int(rm/60)\n",
    "    rm=int(rm%60)\n",
    "    secs = int(rm)\n",
    "    \n",
    "    print (\"\\nTotal Tokens= \", total_tokens)\n",
    "    print (\"\\nStored Tokens= \", stored_tokens)\n",
    "    print(\"\\nIndexing Time : \",hr,\" hrs \",mn,\" mns\",secs,\" secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(inverted_FilePath.format('1'),\"rb\") as f:\n",
    "#     db = pickle.load(f)\n",
    "\n",
    "# print (db)\n",
    "# f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
