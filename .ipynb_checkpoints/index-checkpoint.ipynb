{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.sax\n",
    "import sys, os\n",
    "import csv, re, glob, math, pickle\n",
    "import datetime\n",
    "import Stemmer\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from heapq import heapify, heappush, heappop \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\"])\n",
    "UNWANTED_LINES= set([ \"redirect\", \"align\", \"realign\", \"valign\", \"nonalign\", \"malign\", \"unalign\", \"salign\", \"qalign\", \"halign\", \"font\", \"fontsiz\", \"fontcolor\", \"backgroundcolor\", \"background\", \"style\", \"center\", \"text\"])\n",
    "fields= ['t', 'i', 'c', 'b', 'r', 'l']\n",
    "weights= [60, 40, 30, 50, 20, 10]\n",
    "\n",
    "docId=1\n",
    "current_indexNum= 1\n",
    "final_indexNum= 1\n",
    "max_IndexSize= 100000\n",
    "pageCount= 0\n",
    "\n",
    "Titles= {}\n",
    "Wiki_Dict= {}\n",
    "Doc_Dict= {}\n",
    "Secondary_Index= {}\n",
    "Index_Dict= {}\n",
    "\n",
    "titles_FilePath= \"./Titles\"\n",
    "inverted_FilePath= \"./inverted_index/index{}\"\n",
    "invertedIndex_FilePath= \"./inverted_index/FinalIndex{}\"\n",
    "secondaryIndex_FilePath= \"./SecondaryIndex\"\n",
    "\n",
    "total_tokens =0\n",
    "stored_tokens =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTitles():\n",
    "    titles_file= open(titles_FilePath, \"wb\")\n",
    "    pickle.dump(Titles, titles_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_secondaryIndex():\n",
    "    secondaryIndex_file= open(secondaryIndex_FilePath, \"wb\")\n",
    "    sorted_SecondaryIndex= dict(sorted(Secondary_Index.items(), key=lambda t: t[0]))\n",
    "    pickle.dump(sorted_SecondaryIndex, secondaryIndex_file)  \n",
    "    secondaryIndex_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_PrimaryIndices():\n",
    "    global final_indexNum, stored_tokens\n",
    "    sorted_IndexDict= dict(sorted(Index_Dict.items(), key=lambda t: t[0]))\n",
    "    Secondary_Index[next(iter(sorted_IndexDict))]= str(final_indexNum)\n",
    "\n",
    "    output_file= open(invertedIndex_FilePath.format(str(final_indexNum)), \"wb\")\n",
    "#     for key, value in sorted_IndexDict.items():\n",
    "#         output_file.write(key+ \":\"+ value+ '\\n')\n",
    "    pickle.dump(sorted_IndexDict, output_file) \n",
    "\n",
    "    stored_tokens += len(Index_Dict)\n",
    "    Index_Dict.clear()\n",
    "    final_indexNum +=1\n",
    "    output_file.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kWayMerge():\n",
    "    global current_indexNum\n",
    "    numOfFiles= current_indexNum\n",
    "    input_files= []\n",
    "    \n",
    "    for file_num in range (1,numOfFiles):\n",
    "        input_files.append(open(inverted_FilePath.format(str(file_num)), \"r\"))\n",
    "    \n",
    "    heap = [] \n",
    "    heapify(heap)  \n",
    "    \n",
    "    for file_num in range (1, current_indexNum):\n",
    "        try:\n",
    "            line= input_files[file_num-1].readline()\n",
    "            word= line.split(\":\")[0]\n",
    "            value= line.split(\":\")[1].split(\"\\n\")[0]\n",
    "            heappush(heap, (word, value, file_num))\n",
    "        except:\n",
    "            print (\"No lines is file {}\".format(file_num))\n",
    "            os.remove(inverted_FilePath.format(str(file_num)))\n",
    "    \n",
    "    while heap:\n",
    "        entry= heappop(heap)\n",
    "        word, line, file_num = entry[0], entry[1], entry[2]\n",
    "        \n",
    "        \n",
    "        while heap and heap[0][0] == word:\n",
    "            line += \"|\"+ heap[0][1]\n",
    "            try:\n",
    "                new_line= input_files[heap[0][2]-1].readline()\n",
    "                new_word= new_line.split(\":\")[0]\n",
    "                new_value= new_line.split(\":\")[1].split(\"\\n\")[0]\n",
    "                heappush(heap, (new_word, new_value, heap[0][2]))\n",
    "            except:\n",
    "                print (\"No lines is file {}\".format(heap[0][2]))\n",
    "                os.remove(inverted_FilePath.format(str(heap[0][2])))\n",
    "            heappop(heap)\n",
    "        \n",
    "        Index_Dict[word]= line\n",
    "            \n",
    "        try:\n",
    "            next_line= input_files[file_num-1].readline()\n",
    "            next_word= next_line.split(\":\")[0]\n",
    "            next_value= next_line.split(\":\")[1].split(\"\\n\")[0]\n",
    "            heappush(heap, (next_word, next_value, file_num))\n",
    "        except:\n",
    "            print (\"No lines is file {}\".format(file_num))\n",
    "            os.remove(inverted_FilePath.format(str(file_num)))\n",
    "        \n",
    "        if(len(Index_Dict) == max_IndexSize or len(heap) ==0):\n",
    "            write_PrimaryIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invertedIndex():\n",
    "    global Wiki_Dict, current_indexNum\n",
    "    sorted_WikiDict= sorted(Wiki_Dict.items(), key=lambda t: t[0])\n",
    "    f = open(inverted_FilePath.format(str(current_indexNum)), \"w\")\n",
    "    for key, value in sorted_WikiDict:\n",
    "        line= key +':'+ value +\"\\n\"\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "    current_indexNum +=1\n",
    "    Wiki_Dict.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page:\n",
    "    def ProcessPage(self, pageID, content, index):\n",
    "        global pageCount\n",
    "        for word in content:\n",
    "            if word not in Doc_Dict:\n",
    "                value= np.zeros(6, dtype= int)\n",
    "                Doc_Dict[word]= value\n",
    "                Doc_Dict[word][index] =1\n",
    "            else:\n",
    "                Doc_Dict[word][index] +=1\n",
    "            pageCount +=1\n",
    "\n",
    "        if(index ==5):\n",
    "            for key, value in Doc_Dict.items():\n",
    "                total_frequency =0\n",
    "                if key not in Wiki_Dict:                # pageID_totalcount:t1i2c3b4r5l6|pageID_totalcount:t1i2c3b4r5l6\n",
    "                    Wiki_Dict[key]= str(pageID)+ '-'\n",
    "                    for f in range (0, 6):\n",
    "                        if(value[f] >0):\n",
    "                            Wiki_Dict[key] += fields[f] + str(value[f])\n",
    "                            total_frequency +=value[f]\n",
    "                    Wiki_Dict[key] += 'x'+str(total_frequency)\n",
    "                else:\n",
    "                    Wiki_Dict[key] += '|'+ str(pageID)+ '-'\n",
    "                    for f in range (0, 6):\n",
    "                        if(value[f] >0):\n",
    "                            Wiki_Dict[key] += fields[f] + str(value[f])\n",
    "                            total_frequency +=value[f]\n",
    "                    Wiki_Dict[key] += 'x'+str(total_frequency)\n",
    "            Doc_Dict.clear()   \n",
    "\n",
    "    def extract_pageDetails(self, text):\n",
    "        global total_tokens\n",
    "        lines= text.split('\\n')\n",
    "        line =0\n",
    "        body_flag= False\n",
    "        infoBox, category, body, references, links= [], [], [], [], []\n",
    "\n",
    "        while line <len(lines):\n",
    "            if any(element in lines[line] for element in UNWANTED_LINES) ==True:\n",
    "                total_tokens += len(lines[line].split())\n",
    "                line +=1\n",
    "                \n",
    "            elif \"{{infobox\" in lines[line]:\n",
    "                openBrackets, closedBrackets= 1,0\n",
    "                line +=1\n",
    "                while line< len(lines):\n",
    "                    if \"{{\" in lines[line]:\n",
    "                        openBrackets +=1\n",
    "                    if \"}}\" in lines[line]:\n",
    "                        closedBrackets +=1\n",
    "\n",
    "                    infoBox.extend(lines[line].split())\n",
    "                    if(openBrackets == closedBrackets):\n",
    "                        break\n",
    "                    line +=1\n",
    "                    \n",
    "            elif \"[[category:\" in lines[line]:\n",
    "                body_flag= True\n",
    "                try:\n",
    "                    category.extend((lines[line].split(':')[1].split(']]')[0]).split())\n",
    "                except:\n",
    "                    category.extend((lines[line].split(':')[1]).split())\n",
    "\n",
    "            elif \"==external links==\" in lines[line] or \"== external links ==\" in lines[line]:\n",
    "                body_flag= True\n",
    "                line +=1\n",
    "                while line< len(lines):\n",
    "                    if \"*[\" in lines[line] or \"* [\" in lines[line]:\n",
    "                        link= \"\"\n",
    "                        while line< len(lines) and \"]\" not in lines[line]:\n",
    "                            link += lines[line]\n",
    "                            line+=1\n",
    "                        if(line<len(lines)):\n",
    "                            link += lines[line]\n",
    "                        link = link.split('[')\n",
    "                        if(len(link)> 1):\n",
    "                            link= link[1].split(']')[0]\n",
    "                            links.extend(link.split())\n",
    "                    elif \"[[category:\" in lines[line]:\n",
    "                        try:\n",
    "                            category.extend((lines[line].split(':')[1].split(']]')[0]).split())\n",
    "                        except:\n",
    "                            category.extend((lines[line].split(':')[1]).split())\n",
    "                    line+=1\n",
    "\n",
    "\n",
    "            elif \"==references==\" in lines[line] or \"== references ==\" in lines[line]:\n",
    "                body_flag= True\n",
    "                line +=1\n",
    "                while line< len(lines):\n",
    "                    if \"==\" in lines[line] or \"[[category:\" in lines[line]:\n",
    "                        line -=1\n",
    "                        break\n",
    "                    elif \"{{cite\" in lines[line] or \"{{vcite\" in lines[line]:\n",
    "                        cite_title= lines[line].split(\"title=\")\n",
    "                        if(len(cite_title) >1):\n",
    "                            cite_title= cite_title[1].split(\"|\")[0]\n",
    "                            references.extend(cite_title.split())\n",
    "                    elif \"{{\" in lines[line] and \"ref\" not in lines[line]:\n",
    "                        references.extend((lines[line].split(\"{{\")[1].split(\"}}\")[0]).split())\n",
    "                    line+=1\n",
    "\n",
    "            elif body_flag== False:\n",
    "                body.extend(lines[line].split())\n",
    "            line +=1  \n",
    "\n",
    "        total_tokens+= len(infoBox)+len(category)+len(body)+len(references)+len(links)\n",
    "\n",
    "        return ' '.join(infoBox), ' '.join(category), ' '.join(body), ' '.join(references), ' '.join(links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.docId = 0\n",
    "        self.title=\"\"\n",
    "        self.text = \"\"\n",
    "        self.infoBox= \"\"\n",
    "        self.category= \"\"\n",
    "        self.body= \"\"\n",
    "        self.references= \"\"\n",
    "        self.links= \"\"\n",
    "        self.tokens= 0\n",
    "        self.stemmer= Stemmer.Stemmer(\"english\")\n",
    "\n",
    "    def set_data(self, docId, title, text):\n",
    "        self.docId = docId\n",
    "        self.title= title\n",
    "        self.text = text\n",
    "        \n",
    "        page =Page()\n",
    "        self.infoBox, self.category, self.body, self.references, self.links = page.extract_pageDetails(self.text)\n",
    "        \n",
    "        global total_tokens\n",
    "        total_tokens+= len(self.title.split())\n",
    "        \n",
    "        self.title, self.infoBox, self.category, self.body, self.references, self.links= \\\n",
    "        self.cleanText(self.title), self.cleanText(self.infoBox), self.cleanText(self.category), self.cleanText(self.body), self.cleanText(self.references), self.cleanText(self.links)        \n",
    "                \n",
    "        page.ProcessPage(self.docId, self.title, 0)\n",
    "        page.ProcessPage(self.docId, self.infoBox, 1)\n",
    "        page.ProcessPage(self.docId, self.category, 2)\n",
    "        page.ProcessPage(self.docId, self.body, 3)\n",
    "        page.ProcessPage(self.docId, self.references, 4)\n",
    "        page.ProcessPage(self.docId, self.links, 5)\n",
    "        \n",
    "        global pageCount\n",
    "        self.tokens= pageCount\n",
    "        pageCount= 0\n",
    "        \n",
    "        return self.tokens\n",
    "        \n",
    "    def isEnglish(self, s):\n",
    "        try:\n",
    "            s.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def cleanText(self, text):\n",
    "        text = re.sub(r'<(.*?)>','',text) #Remove tags if any\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE) #Remove Url\n",
    "        text = re.sub(r'{\\|(.*?)\\|}', '', text, flags=re.MULTILINE) #Remove CSS\n",
    "        text = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', text, flags=re.MULTILINE) #Remove File\n",
    "        text = re.sub(r'[^\\w\\s]' , '', text) #Remove Punctuations & Special Characters\n",
    "        text = text.split()\n",
    "        text = [x for x in text if x not in STOPWORDS and x not in URL_STOP_WORDS and (x.isdigit() and (len(x)<=2 or len(x)>=5)) ==False and bool(re.match('^(?=.*[a-zA-Z])(?=.*[0-9])', x)) ==False and self.isEnglish(x) and len(x)<=45] \n",
    "        text = [self.stemmer.stemWord(word) for word in text]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiHandler( xml.sax.ContentHandler ):\n",
    "    def __init__(self):\n",
    "        self.CurrentData = \"\"\n",
    "        self.title = \"\"\n",
    "        self.text = \"\"\n",
    "        self.tokens =0\n",
    "\n",
    "    # Call when an element starts\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.CurrentData = tag\n",
    "\n",
    "    # Call when an elements ends\n",
    "    def endElement(self, tag):\n",
    "        global docId\n",
    "        if self.CurrentData == \"text\":\n",
    "            data= Data()\n",
    "            self.tokens= data.set_data(docId, self.title, self.text)\n",
    "            docId +=1\n",
    "            Titles[docId-1]= (self.title, self.tokens)\n",
    "        self.text= \"\"\n",
    "        self.CurrentData = \"\"\n",
    "\n",
    "    # Call when a character is read\n",
    "    def characters(self, content):\n",
    "        if self.CurrentData == \"title\":\n",
    "            self.title = content.lower()\n",
    "        elif self.CurrentData == \"text\":\n",
    "            self.text += content.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stored Tokens=  0\n",
      "420031 documents were processed\n",
      "--------------------done file1 in 0 hrs 39 mins 21 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "898398 documents were processed\n",
      "--------------------done file2 in 0 hrs 23 mins 17 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "1234805 documents were processed\n",
      "--------------------done file3 in 0 hrs 41 mins 26 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "1355671 documents were processed\n",
      "--------------------done file4 in 0 hrs 23 mins 54 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "1673268 documents were processed\n",
      "--------------------done file5 in 0 hrs 19 mins 28 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "1885093 documents were processed\n",
      "--------------------done file6 in 0 hrs 9 mins 8 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "2076701 documents were processed\n",
      "--------------------done file7 in 0 hrs 15 mins 45 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "2424593 documents were processed\n",
      "--------------------done file8 in 0 hrs 29 mins 55 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "2897383 documents were processed\n",
      "--------------------done file9 in 0 hrs 25 mins 13 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "2981337 documents were processed\n",
      "--------------------done file10 in 0 hrs 19 mins 53 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "3289784 documents were processed\n",
      "--------------------done file11 in 0 hrs 40 mins 11 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "3762529 documents were processed\n",
      "--------------------done file12 in 0 hrs 44 mins 35 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "4213228 documents were processed\n",
      "--------------------done file13 in 0 hrs 33 mins 41 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "4348019 documents were processed\n",
      "--------------------done file14 in 0 hrs 7 mins 34 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "4795829 documents were processed\n",
      "--------------------done file15 in 0 hrs 37 mins 38 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "4991311 documents were processed\n",
      "--------------------done file16 in 0 hrs 36 mins 14 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "5260959 documents were processed\n",
      "--------------------done file17 in 0 hrs 18 mins 31 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "5280756 documents were processed\n",
      "--------------------done file18 in 0 hrs 11 mins 49 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "5674636 documents were processed\n",
      "--------------------done file19 in 0 hrs 42 mins 18 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "5931372 documents were processed\n",
      "--------------------done file20 in 0 hrs 37 mins 24 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "6358004 documents were processed\n",
      "--------------------done file21 in 0 hrs 54 mins 33 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "6756313 documents were processed\n",
      "--------------------done file22 in 0 hrs 52 mins 17 secs--------------\n",
      "\n",
      "Stored Tokens=  0\n",
      "6838603 documents were processed\n",
      "--------------------done file23 in 0 hrs 45 mins 28 secs--------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8b00b7c82046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstart_file\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mcreate_invertedIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mend_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cont_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetDocumentLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpatLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mxmlreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# bpo-30264: Close the source on error to not leak resources:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/xml/sax/xmlreader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data, isFinal)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# document. When feeding chunks, they are not normally final -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# except when invoked from close.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misFinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAXParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../Modules/pyexpat.c\u001b[0m in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mend_element\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cont_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_element_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-a90ea4994b0c>\u001b[0m in \u001b[0;36mendElement\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCurrentData\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mdocId\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mTitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-affc2dabfebe>\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, docId, title, text)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_tokens\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfoBox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m=\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfoBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-affc2dabfebe>\u001b[0m in \u001b[0;36mcleanText\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Remove Punctuations & Special Characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mURL_STOP_WORDS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'^(?=.*[a-zA-Z])(?=.*[0-9])'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-affc2dabfebe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Remove Punctuations & Special Characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mURL_STOP_WORDS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'^(?=.*[a-zA-Z])(?=.*[0-9])'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-affc2dabfebe>\u001b[0m in \u001b[0;36misEnglish\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if ( __name__ == \"__main__\"):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # create an XMLReader\n",
    "    parser = xml.sax.make_parser()\n",
    "    # turn off namepsaces\n",
    "    parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "\n",
    "    # override the default ContextHandler\n",
    "    Handler = WikiHandler()\n",
    "    parser.setContentHandler(Handler)\n",
    "    \n",
    "    files = glob.glob('../Phase2/*.xml') \n",
    "    for file in files:\n",
    "        start_file= datetime.datetime.now()\n",
    "        parser.parse(file)\n",
    "        create_invertedIndex()\n",
    "        end_file = datetime.datetime.now()\n",
    "        secs_file  = (end_file-start_file).seconds\n",
    "        hr_file = int(secs_file/(60*60))\n",
    "        rm_file = int(secs_file%(60*60))\n",
    "        mn_file = int(rm_file/60)\n",
    "        rm_file=int(rm_file%60)\n",
    "        secs_file = int(rm_file)\n",
    "        print()\n",
    "        print (\"Stored Tokens= \", stored_tokens)\n",
    "        print (\"{} documents were processed\".format(docId))\n",
    "        print (\"--------------------done file{} in {} hrs {} mins {} secs--------------\".format(str(current_indexNum-1), str(hr_file), str(mn_file), str(secs_file)))\n",
    "        \n",
    "    \n",
    "    kWayMerge()\n",
    "    write_secondaryIndex()\n",
    "    writeTitles()\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    secs  = (end-start).seconds\n",
    "    hr = int(secs/(60*60))\n",
    "    rm = int(secs%(60*60))\n",
    "    mn = int(rm/60)\n",
    "    rm=int(rm%60)\n",
    "    secs = int(rm)\n",
    "    \n",
    "    print (\"\\nTotal Tokens= \", total_tokens)\n",
    "    print (\"\\nStored Tokens= \", stored_tokens)\n",
    "    print(\"\\nIndexing Time : \",hr,\" hrs \",mn,\" mns\",secs,\" secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
